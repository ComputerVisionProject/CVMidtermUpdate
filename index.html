<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Midterm Update</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name -->
<h1>The Computer Visionaries: Midterm Update</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Rishab Kaup, Karthik Praturu, Noah Sutter and Austen Schunk</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4476 Computer Vision: Final Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Midterm Update -->
<h2><u>Abstract</u></h2>
<!--One or two sentences on the motivation behind the problem you are
solving. One or two sentences describing the approach you took.
One or two sentences on the main result you obtained.-->

Style transfer is a method of transferring the style of an image, like a painting,
to another image. This is useful for creating new types of computer generated
artwork, and it can even be used for videos. Our approach focuses on exploring
and optimizing the current state-of-the-art algorithms for style transfer,
taking pieces of separate algorithms that currently exist, and utilizing the
concepts they introduce to make the general algorithm run faster and more
efficiently for use with videos on everyday computers. We have found key areas
for improvement in the most popular style transfer algorithm currently in use
(shown in [1]) through our experiments and propose a modified structure for
faster performance.

<h3 style = "text-align: center">Teaser Figure</h4>
<div style = "text-align: center">
  <img style = "width: 46%; height:100%" src= "images/style_transfer_ex2.jpg">
  <img style = "width: 47%; height:100%" src= "images/style_transfer_ex1.gif">
</div>

<br>
<br>
<h2><u>Introduction</u></h2>
<!-- Motivation behind the problem you are solving, what applications it has,
any brief background on the particular domain you are working in
(if not regular RBG photographs), etc. If you are using a new way to solve an
existing problem, briefly mention and describe the existing approaches and tell
us how your approach is new. -->

Style transfer algorithms produce very artistic and often seemingly imaginative
outputs, making it a very popular way to filter images and generate abstract
art. Tools like the <a href = "https://deepdreamgenerator.com/" target="_blank">Deep Dream Generator</a>
use style transfer algorithms to generate visual content for consumer use or
research purposes. Real-time style transfer can be used for augmented reality
applications and videos by applying the transformation frame-by-frame, but this
requires faster methods for determining how to transfer style.
<br><br>
The modern style transfer algorithm, introduced by Johnson et al., works as
shown in the following figure:

<br>

<div style = "text-align: center">
  <img style = "width: 46%; height:100%" src= "images/johnsonSTNetwork.png">
</div>
<h5 style = "text-align: center">Johnson et al. Style Tranfer Pipeline [1]</h5>

<br>

The algorithm is split up into two parts, an image transformation network (ITN) and a
loss network. The loss network is a pre-trained VGG16 network meant for
image classification, and it defines weighted functions of feature reconstruction
loss and style reconstruction loss. Most modern style transfer implementations
make use of the same loss network. The loss functions generated by the loss network
are used when training the image transformation network (ITN), and training the
ITN introduced in [1] involves fixing a style image and feeding through a large
database of content images. The ITN introduced by Johnson et al. incorporates a
convolutional neural network with multiple convolutional and pooling layers
throughout; however, other modern implementations use different approaches.
For example, the method introduced in [7] has an ITN that matches the mean
and variance of intermediate features, and the ITN in [8] uses a network that
attempts to minimize the difference of centered covariance between the loss
network output and the combined style and content images.
<br><br>
We approached the problem of style transfer by first taking into account what
currently exists, and then modifying and combining the traits present in each
method to try and achieve an algorithm better suited for modern, high-quality and
fast videos. There were multiple methods we considered, including but not limited
to adding an encoder and decoder network to help speed up transfer times for
high resolution images, incorporating the energy of the content image and the
final image to force the content to still mostly be present, and changing the
ITN introduced by Johnson et al. to include information better suited for videos.
By combining the methods that currently exist and modifying the image
transformation network in [1], we hope we can achieve a new, robust way to
handle style transfer.
<br><br>

<h2><u>Approach</u></h2>
<!--  Describe very clearly and systematically your approach to solve the problem.
Tell us exactly what existing implementations you used to build your system.
Tell us what obstacles you faced and how you addressed them. Justify any design
 choices or judgment calls you made in your approach.-->

  We will be splitting our apporach into two parts. The first part will consist of attempting improving frame rates for CPU video style transfer. The second will seek to improve the quality of real-time video style transfer for GPU.
  <br>
  <h4>Part 1: CPU Improvements</h4>

  Currently, when using the method provided in [1] the frame rate is ~0.6fps depending on the size of the image being transformed, which cannot really be considered "real-time" style transfer. We will attempt to improve this time using the following methods.
  <br>
  <br>

  <!-- Decreasing parameters of Johnson et al. -->
  First, there is the idea of training a more simplistic image transformation network than the one described in [1]. This will be performed based on the idea that a simpler network i.e. one with less parameters will take less computation time. An easy adjustment would be to increase the stride of the convolutions, but this can only be done to a certain limit at which the network vastly decreases in performance. Another option would be to arbitrarily decrease the number of layers, size of filters, or number of filters. While one may get lucky when randomly removing layers, a better idea is to remove model parameters in an informed manner. Removing parameters in this manner is called model compression and while there are many types of model compression, we will use the quantization and pruning technique described in [3].
  <br>
  <br>
  <!-- Using different architecture proposed in  -->
The second idea is to use an architecture that is entirely different: Referenced in [8] this style can be broken into three main parts as viewed in the image below. The first is the VGG encoder / decoder structure. The encoder is the first several layers from the VGG19 network and the decoder is trained to reproduce the images that are passed through this encoder. The purpose of this structure is to help reproduce any style / content combination that might be produced by applying a transformation to the output of the encoder. The second part is the loss module that is essentially the same as the one described in [1]. This module computes both the style loss using the difference in Gram matrices between layers and the content loss using the difference in feature between the input and output images. Finally, the third part is the transformation module. This module takes is broken up into two networks that combine at the end of the transformation module to produce a transformation matrix. These two networks take in either the feature vector of the content or the feature vector of the style then runs them through a three-layer deep Convolutional Neural Network (CNN). Once it has done this it computes the covariance of these matrices and feeds them into a fully connected neural network. The outputs of the two networks are then multiplied together to produce the transformation matrix. This matrix is then applied to the feature vector of the content which the decoder will then transform into a final image. Training this module takes places after the decoder has been trained by adding a combination of the content and style losses to both of the networks that make up the transformation module. This whole network is extremely effective in speeding up the style transfer because it reduces the number of layers needed as well as removing the need to train a network for every style of image. It also speeds up the style transfer for videos or real-time style transfer by only needing to compute one half of the transformation matrix (the style half) once. After being computed the first time it can simply be stored further speeding up the approach. While this method has not been implemented by anyone else or confirmed by other research papers as it came out around a month ago, we will implement this method in order to hopefully achieve real time style transfer on the CPU.    <br>
<br>
<div style = "text-align: center">
  <img style = "width: 50%; height:100%" src= "images/arbitrary_method.png">
</div>

  <br>
  <h4>Part 2: GPU Improvements</h4>

  Contrary to real-time style transfer on a CPU, real-time style transfer on a GPU is more or less solved. As a result, The goal in this setting will be to reduce a sort of flickering effect that occurs as a result of temporal inconsistencies between frames. An example of this can be seen in the teaser figure in which one can basically identify the change of frame. The "video" comes as a result of applying the transformation network from [1] repeatedly on each frame. The main problem with this is there is no concept of consistency between frames. The can be solved by using adding a temporal loss with the style and content loss. This loss is one that enforces a penalty for changes between output frames. We will be implementing two models that attempt to use this idea of a temporal loss. The first is the model presented in [4] which passes the image at time t and t-1 into an image transformation network consecutively then uses the per pixel MSE(Mean-Squared-Error) of the image at time t and a function(tries to map the image from t-1 to t) of t-1. The model then calculates the style and content loss at time t in the same manner as [1], but now combines that spatial loss with this new temporal loss in order to update the network. 
  
  <br>
  <div style = "text-align: center">
    <img style = "width: 50%; height:100%" src= "images/Huang_method.png">
  </div>
  <br>
  
  The second model which we will implement is the one proposed in [5]. In this model, we use an encoder-decoder network rather than the conventional feed-forward convolutional neural network. The main difference between this network and the one in [4] is that the temporal loss will be computed on the features that are encoded by the encoder rather than on the generated image.

  <br>
  <br>
  Another approach that we will take to attempt to solve this flickering problem should also be able to be solved by the method proposed in [8]. By using the covariance of the feature vectors produced by the encoder to use as input to the Convolutional Neural Networks (CNNs)  the proposed method is able to preserve content affinity while maintaining its quick runtimes. This ignores temporal consistencies in favor of attempting to transform features the same way without regard to where they are in an image. 


  <br>
  <br>
  <b>Note:</b> Across all proposed models above, there will be the general goal to sacriface small losses in performance by using a smaller model in order to increase fps. Additionally, the models specified in the GPU portion will also be tested for cpu performance but not the other way around.



<br><br>
<!-- Experiments and Results -->
<h2><u>Current Experiments and Results</u></h3>
  For this midterm update, we decided to focus on implementing and testing the current approach detailed in [1]. We trained a neural network from scratch
  with this approach on the following style images, using the Microsoft COCO dataset as described in [2]:
  <br>
  <br>

  <div style = "text-align: center">
    <img style = "width: 50%; height:100%" src= "images/lionStyle.jpg">
    <img style = "width: 37%; height:100%" src= "images/stainedStyle.jpg">
  </div>

  <br>
  <br>

  After training, which took 4-6 hours using a GTX 1080ti, the results for two test images are shown below:

  <br>
  <br>

  <div style = "text-align: center">
    <img style = "width: 45%; height:100%" src= "images/chicago.jpg">
    <img style = "width: 45%; height:100%" src= "images/stata.jpg">
  </div>
  <br><br>

  <div style = "text-align: center">
    <img style = "width: 45%; height:100%" src= "images/chicagoLion.jpg">
    <img style = "width: 45%; height:100%" src= "images/chicagoStained.jpg">
  </div>
  <br><br>
  <div style = "text-align: center">
    <img style = "width: 45%; height:100%" src= "images/stataLion.jpg">
    <img style = "width: 45%; height:100%" src= "images/stataStained.jpg">
  </div>

  <br>
  <br>

  As you can see, the results are very good, both keeping the style of the style image, and the content of the original image. This was expected expected for this proven method.

  <br>
  <br>
  However, stylization is not the result we are most interested. The timing results for running the neural net on several resolutions are very important for video transfer, and are shown below. These results were run on a CPU rather than a GPU, as we will test GPU performance on videos directly against our improved method once completed.
  The images are static images, and can be extrapolated to assume that video performance would be based on the static conversion times.

  <div style = "text-align: center">
    <img style = "width: 100%; height:100%" src= "images/runtime_average.png">
  </div>

  This performance is not even close to the speeds that are needed for real time video transfer at an acceptable frame rate. Obviously larger images offered poorer conversion times, but even small 256x256 images averaged around ~1.5s, yielding a measly estimated 0.66 frames per second
<br><br>
<h2><u>Future Experimentation</u></h2>
  <h4>Quantitative Experimentation</h4>
  The goals that we have set in the approach section above will be tested in several different ways with quantitative results:
  <h5>For both image and video</h5>
  <ul>
    <li>Time trials for training on style images
    	<ul>
    		<li>Directly compare times for training using the style transfer described in [1] and the style transfer changes we implement from [2] and our own ideas</li>
    	</ul>
    </li>
    <li>Time trials for testing on static content images
    	<ul>
    		<li>Directly compare times for testing using the style transfer described in [1] and the style transfer changes we implement from [2] and our own ideas</li>
    	</ul>
    </li>
  </ul>

  <h5>For video only</h5>
  <ul>
    <li>Frame rate trials for flickering/smoothness
    	<ul>
    		<li>Directly compare frame rate from videos created using [1] method, and new method</ul>
    </li>
  </ul>

  <br>
  <h4>Qualitative Experimentation</h4>
   In the future qualitative experiments, we will give the users/subjects pictures and videos, with ranging complexities, 
   to have the style transferred from the original image/video to that of a given template. In this context, the complexity of an image will be measured using the "energy" of an image
   Given the largely subjective nature of evaluating the quality of a style transfer, for each image we will ask, in a survey, 
   for users/subjects to rate the style transfer on a scale of 1 to 5 for the following criteria.


  <h5>For both image and video</h5>
  <ul>
    <li>How acceptable is the time taken for processing the style transfer
    	<ul>
    		<li>5 being the most acceptable and 1 being the least acceptable</li>
    	</ul>
    </li>
    <li>Overall quality of image produced
    	<ul>
    		<li>5 being the best quality and 1 being the worst quality</li>
    	</ul>
    </li>
    <li>How related the style of the generated image is with the desired style
    	<ul>
    		<li>5 being the generated style completely matches the desired style and 1 being the generated style does not match the desired style at all</li>
    	</ul>
    </li>
    <li>How related the content of the generated image is with the original
    	<ul>
    		<li>5 being the content of the generated image is completely related to the content of the original image and 1 being the content of the generated image is not related to the content of the original image</li>
    	</ul>
    </li>
  </ul>
  <h5>For only video</h5>
  <ul>
    <li>Overall continuity of the video i.e. was the video skipping significantly between frames? not maintining a certain style? etc..
    	<ul>
    		<li>5 being perfect continuity and 1 being no  continuity</li>
    	</ul>
    </li>
  </ul>
  Through these experiments we hope to learn the strengths and weakness of the different techniques and hopefully determine which is best in general for transferring a style onto an image or video. Overall, this experiment will be considered a success if we are able to implement an application that is able to perform real-time style transfer for videos and receive relatively good reviews from our users (an average of 3.5 or above in each section of the survey).
<br>
<br>

<h2><u>Conclusion</u></h2>

Obviously we have much work to do in the upcoming weeks, but we are optimistic that we will be able to succeed in creating stunning visual art that is both efficient and beautiful using the above approach
and validate those results with the described experimentation.


<br><br><br>

<hr>

Relevant papers and articles:
<br>
<ol>
  <li>Johnson et al. <a href = "https://arxiv.org/pdf/1603.08155.pdf" target="_blank">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></li>
  <li>Ruder et al. <a href = "https://arxiv.org/pdf/1604.08610.pdf" target="_blank">Artistic Style Transfer for Video</a></li>
  <li>Cheng et al. <a href="https://arxiv.org/pdf/1710.09282.pdf">A Survey of Model Compression and Acceleration for Deep Neural Networks</a></li>
  <li>Huang et al. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf">
    Real-Time Neural Style Transfer for Videos
  </a></li>
  <li>Gao et al. <a href="https://arxiv.org/pdf/1807.01197.pdf">ReCoNet: Real-time Coherent Video StyleTransfer Network</a></li>
  <li>Weinzaepfel et al. <a href="https://hal.inria.fr/hal-00873592/document">DeepFlow: Large displacement optical flow with deep matching</a></li>
  <li>Huang et al. <a href="https://arxiv.org/pdf/1703.06868.pdf" >Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</a></li>
  <li>Li et al. <a href="https://arxiv.org/pdf/1808.04537v1.pdf">Learning Linear Transformations for Fast Arbitrary Style Transfer</a></li>
</ol>
<br>


<!-- When \(a \ne 0\), there are two solutions to \(ax^2 + bx + c = 0\) and they are
$$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$
<br>
Experiments and results: Describe the experimental setup you will follow, which datasets you will use, which existing code you will exploit, what you will implement yourself, and what you would define as a success for the project. If you plan on collecting your own data, describe what data collection protocol you will follow. Provide a list of experiments you will perform. Describe what you expect the experiments to reveal, or what is uncertain about the potential outcomes.
 -->
  <hr>
  <footer>
  <p>© Rishab Kaup, Karthik Praturu, Noah Sutter and Austen Schunk</p>
  </footer>
</div>
</div>

</body></html>
